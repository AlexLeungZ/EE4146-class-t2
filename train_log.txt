python -u ./train.py --resume 'checkpoints/densenet121-a639ec97.pth'

Total # images:7876, labels:7876
Total # images:2624, labels:2624
number of features in block1 256
number of features in block2 512
number of features in block3 1024
number of features in block4 1024
C:\Users\chunc\.conda\envs\dlearn\lib\site-packages\torch\optim\lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
C:\Users\chunc\.conda\envs\dlearn\lib\site-packages\torch\optim\lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)

===> epoch: 1/20
train:
 493/493 [================================================================================>]  Step: 1s246ms | Tot: 1m11s | Loss: 0.5689 | Acc: 80.447% (6336/7876)
(280.467438288033, 0.8044692737430168)
test:
 164/164 [================================================================================>]  Step: 51ms | Tot: 8s590ms | Loss: 0.3555 | Acc: 88.758% (2329/2624)

===> epoch: 2/20
train:
 493/493 [================================================================================>]  Step: 136ms | Tot: 1m11s | Loss: 0.4152 | Acc: 86.173% (6787/7876)))
(204.68630079180002, 0.861731843575419)
test:
 164/164 [================================================================================>]  Step: 52ms | Tot: 8s859ms | Loss: 0.3125 | Acc: 89.482% (2348/2624)

===> epoch: 3/20
train:
 493/493 [================================================================================>]  Step: 127ms | Tot: 1m11s | Loss: 0.3610 | Acc: 87.329% (6878/7876)))
(177.95867235399783, 0.8732859319451498)
test:
 164/164 [================================================================================>]  Step: 48ms | Tot: 8s795ms | Loss: 0.4097 | Acc: 86.509% (2270/2624)

===> epoch: 4/20
train:
 493/493 [================================================================================>]  Step: 130ms | Tot: 1m11s | Loss: 0.3189 | Acc: 88.878% (7000/7876)))
(157.23301779199392, 0.8887760284408329)
test:
 164/164 [================================================================================>]  Step: 48ms | Tot: 8s793ms | Loss: 0.4415 | Acc: 84.756% (2224/2624)

===> epoch: 5/20
train:
 493/493 [================================================================================>]  Step: 134ms | Tot: 1m11s | Loss: 0.2842 | Acc: 90.109% (7097/7876)))
(140.10820877272636, 0.9010919248349416)
test:
 164/164 [================================================================================>]  Step: 49ms | Tot: 9s586ms | Loss: 0.2724 | Acc: 90.358% (2371/2624)

===> epoch: 6/20
train:
 493/493 [================================================================================>]  Step: 117ms | Tot: 1m11s | Loss: 0.2774 | Acc: 90.223% (7106/7876)))
(136.7815248593688, 0.9022346368715084)
test:
 164/164 [================================================================================>]  Step: 48ms | Tot: 8s672ms | Loss: 0.3090 | Acc: 89.062% (2337/2624)

===> epoch: 7/20
train:
 493/493 [================================================================================>]  Step: 137ms | Tot: 1m11s | Loss: 0.2562 | Acc: 91.353% (7195/7876)))
(126.31840685941279, 0.9135347892331133)
test:
 164/164 [================================================================================>]  Step: 54ms | Tot: 8s948ms | Loss: 0.2841 | Acc: 90.168% (2366/2624)

===> epoch: 8/20
train:
 493/493 [================================================================================>]  Step: 114ms | Tot: 1m12s | Loss: 0.2498 | Acc: 91.366% (7196/7876)))
(123.15373024344444, 0.9136617572371762)
test:
 164/164 [================================================================================>]  Step: 50ms | Tot: 8s799ms | Loss: 0.3500 | Acc: 88.415% (2320/2624)

===> epoch: 9/20
train:
 493/493 [================================================================================>]  Step: 124ms | Tot: 1m10s | Loss: 0.2273 | Acc: 91.976% (7244/7876)))
(112.08151750639081, 0.919756221432199)
test:
 164/164 [================================================================================>]  Step: 56ms | Tot: 8s795ms | Loss: 0.4382 | Acc: 85.671% (2248/2624)

===> epoch: 10/20
train:
 493/493 [================================================================================>]  Step: 137ms | Tot: 1m11s | Loss: 0.2079 | Acc: 92.357% (7274/7876)))
(102.50195172289386, 0.9235652615540884)
test:
 164/164 [================================================================================>]  Step: 51ms | Tot: 8s547ms | Loss: 0.2692 | Acc: 90.206% (2367/2624)

===> epoch: 11/20
train:
 493/493 [================================================================================>]  Step: 115ms | Tot: 1m11s | Loss: 0.2116 | Acc: 92.776% (7307/7876)))
(104.3372808531858, 0.9277552056881666)
test:
 164/164 [================================================================================>]  Step: 51ms | Tot: 8s793ms | Loss: 0.2949 | Acc: 90.549% (2376/2624)

===> epoch: 12/20
train:
 493/493 [================================================================================>]  Step: 117ms | Tot: 1m11s | Loss: 0.1869 | Acc: 93.487% (7363/7876)))
(92.15085678640753, 0.9348654139156932)
test:
 164/164 [================================================================================>]  Step: 50ms | Tot: 8s758ms | Loss: 0.3253 | Acc: 89.596% (2351/2624)

===> epoch: 13/20
train:
 493/493 [================================================================================>]  Step: 133ms | Tot: 1m11s | Loss: 0.1700 | Acc: 93.956% (7400/7876)))
(83.82887885347009, 0.9395632300660234)
test:
 164/164 [================================================================================>]  Step: 49ms | Tot: 8s685ms | Loss: 0.3444 | Acc: 89.215% (2341/2624)

===> epoch: 14/20
train:
 493/493 [================================================================================>]  Step: 123ms | Tot: 1m12s | Loss: 0.1674 | Acc: 94.147% (7415/7876)))
(82.51316716661677, 0.941467750126968)
test:
 164/164 [================================================================================>]  Step: 55ms | Tot: 8s446ms | Loss: 0.2283 | Acc: 92.302% (2422/2624)

===> epoch: 15/20
train:
 493/493 [================================================================================>]  Step: 126ms | Tot: 1m10s | Loss: 0.1527 | Acc: 94.604% (7451/7876)))
(75.28376173763536, 0.9460385982732351)
test:
 164/164 [================================================================================>]  Step: 48ms | Tot: 8s442ms | Loss: 0.2980 | Acc: 90.663% (2379/2624)

===> epoch: 16/20
train:
 493/493 [================================================================================>]  Step: 130ms | Tot: 1m10s | Loss: 0.1378 | Acc: 94.731% (7461/7876)))
(67.958338170778, 0.9473082783138649)
test:
 164/164 [================================================================================>]  Step: 54ms | Tot: 8s439ms | Loss: 0.3055 | Acc: 90.854% (2384/2624)

===> epoch: 17/20
train:
 493/493 [================================================================================>]  Step: 119ms | Tot: 1m10s | Loss: 0.1485 | Acc: 94.718% (7460/7876)))
(73.19306671968661, 0.9471813103098019)
test:
 164/164 [================================================================================>]  Step: 47ms | Tot: 8s573ms | Loss: 0.3180 | Acc: 90.130% (2365/2624)

===> epoch: 18/20
train:
 493/493 [================================================================================>]  Step: 113ms | Tot: 1m10s | Loss: 0.1308 | Acc: 95.302% (7506/7876)))
(64.49883723037783, 0.9530218384966989)
test:
 164/164 [================================================================================>]  Step: 55ms | Tot: 8s399ms | Loss: 0.4379 | Acc: 88.834% (2331/2624)

===> epoch: 19/20
train:
 493/493 [================================================================================>]  Step: 127ms | Tot: 1m10s | Loss: 0.1183 | Acc: 95.708% (7538/7876)))
(58.30628641822841, 0.957084814626714)
test:
 164/164 [================================================================================>]  Step: 48ms | Tot: 8s292ms | Loss: 0.3567 | Acc: 89.901% (2359/2624)

===> epoch: 20/20
train:
 493/493 [================================================================================>]  Step: 125ms | Tot: 1m10s | Loss: 0.1162 | Acc: 95.962% (7558/7876)))
(57.30757537379395, 0.9596241747079736)
test:
 164/164 [================================================================================>]  Step: 57ms | Tot: 8s374ms | Loss: 0.3838 | Acc: 89.024% (2336/2624)
===> BEST ACC. PERFORMANCE: 92.302%
Checkpoint saved to ./checkpoints/model.pth